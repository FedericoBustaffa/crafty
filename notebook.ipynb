{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRAFTY\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Importo i dataset con `pandas` cercando di risparmiare memoria andando a specificare, tramite\n",
    "il parametro `dtype` il tipo e la dimensione dei dati di ogni colonna.\n",
    "\n",
    "In questo modo risparmiao quasi 1GB di RAM e velocizzo il caricamento del dataset. Sono stato in\n",
    "parte costretto a procedere in questo modo poiché non sempre riuscivo a contenere tutti i dati\n",
    "generati nella RAM della macchina.\n",
    "\n",
    "Più avanti andrò inoltre a deallocare i DataFrame più grandi una volta che questi non sono più necessari.\n",
    "Questo potrebbe creare qualche problema nel caso si voglia eseguire tutto il notebook e in seguito eseguire\n",
    "qualche cella specifica poiché si potrebbe far riferimento a dati già deallocati.\n",
    "\n",
    "Ovviamente eliminando o commentando le righe di codice in cui viene fatta la deallocazione non si avrebbe\n",
    "questo problema ma nel mio caso è stato necessario per riuscire ad eseguire il notebook senza saturare la RAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import bs4\n",
    "import requests\n",
    "import time\n",
    "\n",
    "n = None\n",
    "\n",
    "tx_df = pd.read_csv(\n",
    "    \"transactions.csv\",\n",
    "    names=[\"timestamp\", \"blk\", \"tx\", \"is_coinbase\", \"fee\"],\n",
    "    dtype={\n",
    "        \"timestamp\": np.uint32,\n",
    "        \"blk\": np.uint32,\n",
    "        \"tx\": np.uint32,\n",
    "        \"is_coinbase\": np.uint8,\n",
    "        \"fee\": np.uint64,\n",
    "    },\n",
    "    nrows=n,\n",
    ")\n",
    "tx_df[\"timestamp\"] = pd.to_datetime(tx_df[\"timestamp\"], unit=\"s\")\n",
    "tx_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi\n",
    "\n",
    "In questa prima fase andiamo ad effettuare delle analisi generali sul dataset.\n",
    "\n",
    "### Distribuzione del numero di transazioni per blocco\n",
    "\n",
    "Vogliamo ricavare la **distribuzione** del numero di transazioni per blocco, nell'intero periodo\n",
    "temporale considerato.\n",
    "\n",
    "Per prima cosa raggruppiamo il dataset per blocco.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "blk_grp = tx_df.groupby(\"blk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dato che ogni riga del dataset rappresenta una transazione e dato che ogni transazione ha un\n",
    "identificatore univoco, questa comparirà al più una volta nel dataset.\n",
    "\n",
    "Siamo quindi in grado di ottenere il numero di transazioni per blocco tramite la funzione di\n",
    "aggregazione `count`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tx_per_blk = blk_grp[\"tx\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "\n",
    "Come sistema di plotting per la distribuzione del numero di transazioni per blocco ho fornito due\n",
    "punti di vista:\n",
    "\n",
    "- Istogramma delle frequenze di ogni dimensione dei blocchi trovata.\n",
    "- Distribuzione della dimensione dei blocchi nel tempo con un grafico a barre approssimato.\n",
    "\n",
    "Per plottare la seconda distribuzione ho optato per un grafico a barre ma plottare una barra per\n",
    "ogni blocco (quasi 200000 blocchi) sarebbe stato troppo lungo.\n",
    "\n",
    "Ho quindi deciso di raggruppare il dataset ulteriormente. Dato che volevo 30 barre avevo bisogno di\n",
    "raggruppare ogni $n$ righe. Dove $n$ viene calcolato dividendo la lunghezza del dataset per 30.\n",
    "\n",
    "Ho infine calcolato la media di ogni gruppo ottenuto per avere una rappresentazione del numero di transazioni\n",
    "per blocco.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "ax1.hist(tx_per_blk, bins=20, ec=\"w\")\n",
    "ax1.set_title(\"Distribuzione delle transazioni per blocco\")\n",
    "ax1.set_xlabel(\"Dimensione del blocco\")\n",
    "ax1.set_ylabel(\"Quantità di blocchi\")\n",
    "ax1.set_yscale(\"log\")\n",
    "\n",
    "bins = 30\n",
    "group_size = len(tx_per_blk) // bins\n",
    "tx_per_blk_approx = tx_per_blk.groupby(np.arange(len(tx_per_blk)) // group_size).mean()\n",
    "\n",
    "ax2.bar(\n",
    "    tx_per_blk_approx.index,\n",
    "    tx_per_blk_approx,\n",
    "    color=\"tab:orange\",\n",
    "    label=\"Transazioni\",\n",
    ")\n",
    "ax2.set_title(\"Distribuzione delle transazioni per blocco nel tempo\")\n",
    "ax2.set_xlabel(\"Tempo\")\n",
    "ax2.set_ylabel(\"Transazioni per blocco\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = tx_per_blk.index.to_list()\n",
    "y = tx_per_blk.values.tolist()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y, label=\"transazioni per blocco\")\n",
    "\n",
    "plt.title(\"Distribuzione delle transazioni per blocco\")\n",
    "plt.xlabel(\"ID del blocco\")\n",
    "plt.ylabel(\"Transazioni per blocco\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occupazione dei blocchi nel tempo\n",
    "\n",
    "La seconda analisi prevede lo studio di come l'occupazione dei blocchi si è evoluta nel tempo.\n",
    "\n",
    "In particolare dobbiamo andare a considerare intervalli di due mesi.\n",
    "\n",
    "Dato che avevamo già raggruppato il dataset per blocchi, ho deciso di considerare come data di\n",
    "riferimento, la data della transazione meno recente come data di riferimento di ogni blocco.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transazioni per blocco con timestamp\n",
    "tx_per_blk = pd.concat([tx_per_blk, blk_grp[\"timestamp\"].min()], axis=\"columns\")\n",
    "tx_per_blk = tx_per_blk.rename(columns={\"tx\": \"tx_count\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ho infine utilizzato l'oggetto `Grouper` di Pandas per riuscire ad interagire comodamente con le\n",
    "date ed effettuare un raggruppamento bimestrale dei blocchi.\n",
    "\n",
    "Per ogni gruppo ho infine calcolato l'occupazione media dei blocchi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_per_blk = (\n",
    "    tx_per_blk.groupby(pd.Grouper(key=\"timestamp\", freq=\"2MS\"))[\"tx_count\"]\n",
    "    .mean()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "\n",
    "Come metodologia di plotting ho optato per un semplice plot in grado di fornire una rappresentazione\n",
    "accurata dell'occupazione media dei blocchi ogni due mesi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(\n",
    "    tx_per_blk[\"timestamp\"],\n",
    "    tx_per_blk[\"tx_count\"],\n",
    "    c=\"tab:red\",\n",
    "    marker=\"o\",\n",
    "    mec=\"k\",\n",
    "    label=\"transazioni medie per blocco\",\n",
    ")\n",
    "\n",
    "plt.title(\"Occupazione media dei blocchi ogni 2 mesi\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Numero medio di transazioni per blocco\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Totale UTXO\n",
    "\n",
    "Passiamo ora a calcolare l'ammontare totale degli **UTXO** al momento dell'ultima transazione\n",
    "registrata nel dataset.\n",
    "\n",
    "L'idea è quella di calcolare prima tutto l'output che è stato speso e poi sottrarlo dall'output\n",
    "totale prodotto.\n",
    "\n",
    "Per arrivare a questo risultato effettuiamo un'operazione di `merge` tra il dataset degli **input**\n",
    "e il dataset degli **output**.\n",
    "\n",
    "Dato che non abbiamo identificatori univoci per ogni output ma solo per le transazioni, per\n",
    "calcolare l'UTXO totale dobbiamo:\n",
    "\n",
    "- Considerare le transazioni mai riferite come input da altre transazioni.\n",
    "- Considerare gli output mai riferiti da altre transazioni che però riferiscono altri output\n",
    "  all'interno della stessa transazione.\n",
    "\n",
    "Ho deciso quindi di filtrare il dataset degli output eliminando\n",
    "\n",
    "Ho deciso quindi di implementare l'operazione di `merge` (in questo caso di tipo _inner_) basandomi\n",
    "sia sulle transazioni che sugli output riferiti in input.\n",
    "\n",
    "Il risultato sarà un DataFrame `spent_df` in cui ogni riga è identificata univocamente dalla coppia\n",
    "`(in_tx, out_pos)`.\n",
    "\n",
    "In questo modo otterremo un dataset in cui saranno presenti solo le transazioni riferite come input\n",
    "da altre transazioni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df = pd.read_csv(\n",
    "    \"inputs.csv\",\n",
    "    names=[\"tx\", \"utxo_tx\", \"utxo_out_pos\"],\n",
    "    dtype={\"tx\": np.uint32, \"utxo_tx\": np.uint32, \"out_pos\": np.uint16},\n",
    "    nrows=n,\n",
    ")\n",
    "\n",
    "out_df = pd.read_csv(\n",
    "    \"outputs.csv\",\n",
    "    names=[\"tx\", \"out_pos\", \"address\", \"amount\", \"script_type\"],\n",
    "    dtype={\n",
    "        \"tx\": np.uint32,\n",
    "        \"out_pos\": np.uint16,\n",
    "        \"address\": np.uint32,\n",
    "        \"amount\": np.uint64,\n",
    "        \"script_type\": np.uint8,\n",
    "    },\n",
    "    nrows=n,\n",
    ")\n",
    "\n",
    "spent_df = (\n",
    "    out_df.merge(\n",
    "        in_df,\n",
    "        how=\"inner\",\n",
    "        left_on=[\"tx\", \"out_pos\"],\n",
    "        right_on=[\"utxo_tx\", \"utxo_out_pos\"],\n",
    "    )\n",
    "    .rename(columns={\"tx_x\": \"in_tx\", \"tx_y\": \"tx\"})\n",
    "    .drop(columns=[\"utxo_tx\", \"utxo_out_pos\", \"script_type\"])\n",
    ")\n",
    "\n",
    "del in_df\n",
    "\n",
    "spent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto possiamo sommare i valori della colonna `amount` del DataFrame `spent_df` per\n",
    "ricavare l'output speso.\n",
    "\n",
    "Se invece sommiamo i valori della colonna `amount` del dataset degli **output** generati in tutto\n",
    "il periodo di tempo considerato ricaviamo il valore totale degli output sia speso che non speso.\n",
    "\n",
    "Calcolando la differenza tra il il valore totale degli output e il valore totale degli output spesi\n",
    "ricaviamo l'**UTXO** totale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utxo = out_df[\"amount\"].sum() - spent_df[\"amount\"].sum()\n",
    "print(f\"UTXO totale: {utxo}\")\n",
    "\n",
    "del out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "\n",
    "Tramite un grafico a torta è possibile comparare l'output speso con l'**UTXO**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.pie(\n",
    "    [utxo, spent_df[\"amount\"].sum()],\n",
    "    explode=(0.1, 0),\n",
    "    labels=[\"UTXO\", \"Totale speso\"],\n",
    "    colors=[\"tab:green\", \"tab:red\"],\n",
    "    autopct=\"%.2f%%\",\n",
    ")\n",
    "\n",
    "plt.title(\"UTXO\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuzione intervalli di tempo per il consumo di un output\n",
    "\n",
    "Abbiamo già a disposizione un DataFrame che mette in correlazione gli input e gli output.\n",
    "\n",
    "Per capire dopo quanto tempo è stato speso un output dobbiamo aggiungere il valore di `timestamp`\n",
    "sia per il valore in input che per il valore in output.\n",
    "\n",
    "Per riuscire a farlo ho effettuato un doppio `merge` con il DataFrame delle transazioni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spent_time_df = (\n",
    "    spent_df.merge(tx_df, how=\"inner\", on=\"tx\")\n",
    "    .drop(columns=[\"blk\", \"tx\", \"is_coinbase\", \"fee\"])\n",
    "    .rename(columns={\"timestamp\": \"spent_time\"})\n",
    ")\n",
    "\n",
    "spent_time_df = (\n",
    "    spent_time_df.merge(tx_df, how=\"inner\", left_on=\"in_tx\", right_on=\"tx\")\n",
    "    .drop(columns=[\"blk\", \"tx\", \"is_coinbase\", \"fee\"])\n",
    "    .rename(columns={\"timestamp\": \"gen_time\"})\n",
    ")\n",
    "\n",
    "days = (spent_time_df[\"spent_time\"] - spent_time_df[\"gen_time\"]).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "\n",
    "A questo punto abbiamo una Series `days`, contenente il numero di giorni di vita di ogni transazione.\n",
    "\n",
    "Sull'asse $x$ abbiamo la lunghezza degli intervalli di tempo (in giorni) prima che un output\n",
    "venisse speso.\n",
    "\n",
    "Sull'asse $y$ abbiamo invece la quantità di output che vengono spesi ogni $x$ giorni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(days, bins=50, ec=\"w\", color=\"teal\")\n",
    "\n",
    "plt.title(\"Distribuzione degli intervalli di tempo\")\n",
    "plt.xlabel(\"Giorni prima di spendere l'output\")\n",
    "plt.ylabel(\"Quantità di output\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuzione delle fee\n",
    "\n",
    "In quest'ultima fase vogliamo fare due analisi\n",
    "\n",
    "1. Ricavare la distribuzione del numero di _fee_ e il valor medio di _fee_ al mese.\n",
    "2. Mettere a confronto la distribuzione di _fee_ nulle e la distribuzione di _fee_ non nulle nel\n",
    "   tempo.\n",
    "\n",
    "Per prima cosa raggruppiamo il dataset per mese e ricaviamo il numero di _fee_ e il valor medio di\n",
    "mensile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_grp = tx_df.groupby(pd.Grouper(key=\"timestamp\", freq=\"1MS\"))\n",
    "fee_per_month = ts_grp[\"fee\"].count()\n",
    "fee_value_per_month = ts_grp[\"fee\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo ora dividere il dataset in due parti:\n",
    "\n",
    "- Transazioni con _fee_ nulle.\n",
    "- Transazioni con _fee_ non nulle.\n",
    "\n",
    "Una volta separati andiamo a raggruppare nuovamente per mese e calcoliamo, per entrambi i dataset\n",
    "ricavati, il numero di _fee_ mensile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_fee = tx_df[tx_df[\"fee\"] == 0]\n",
    "non_zero_fee = tx_df[tx_df[\"fee\"] > 0]\n",
    "\n",
    "zero_fee_grp = zero_fee.groupby(pd.Grouper(key=\"timestamp\", freq=\"1MS\"))\n",
    "zero_fee_count = zero_fee_grp[\"fee\"].count()\n",
    "\n",
    "non_zero_fee_grp = non_zero_fee.groupby(pd.Grouper(key=\"timestamp\", freq=\"1MS\"))\n",
    "non_zero_fee_count = non_zero_fee_grp[\"fee\"].count()\n",
    "\n",
    "del tx_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "\n",
    "Dato che abbiamo calcolato due distribuzioni ho optato per effettuare un doppio plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = fee_per_month.index.to_list()\n",
    "y1 = fee_per_month.to_list()\n",
    "y2 = fee_value_per_month.to_list()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12))\n",
    "\n",
    "ax1.plot(x, y1, \"o-\", mec=\"k\", label=\"quantità media mensile delle fee\", c=\"seagreen\")\n",
    "ax1.plot(x, y2, \"o-\", mec=\"k\", label=\"valor medio mensile delle fee\", c=\"coral\")\n",
    "ax1.set_title(\"Distribuzione quantità e valor medio delle fee\")\n",
    "ax1.set_xlabel(\"Data\")\n",
    "ax1.set_xticks([ts for ts in x if ts.month % 6 == 0])\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "x = zero_fee_count.index.to_list()\n",
    "y = zero_fee_count.tolist()\n",
    "ax2.plot(x, y, \"o-\", mec=\"k\", label=\"quantità fee nulle\", c=\"mediumslateblue\")\n",
    "\n",
    "x = non_zero_fee_count.index.to_list()\n",
    "y = non_zero_fee_count.tolist()\n",
    "ax2.plot(x, y, \"o-\", mec=\"k\", label=\"quantità fee non nulle\", c=\"blueviolet\")\n",
    "ax2.set_title(\"Distribuzione fee nulle e non nulle\")\n",
    "ax2.set_xlabel(\"Data\")\n",
    "ax2.set_ylabel(\"Numero di fee\")\n",
    "ax2.set_xticks([ts for ts in x if ts.month % 6 == 0])\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterizzazione degli indirizzi\n",
    "\n",
    "In questa fase vogliamo implementare l'**euristica multi-input** per la clusterizzazione degli\n",
    "indirizzi.\n",
    "\n",
    "L'obbiettivo è quello di riunire in cluster, gli indirizzi appartenenti ad uno stesso utente.\n",
    "\n",
    "### Grafo degli indirizzi\n",
    "\n",
    "Per costruire il grafo degli indirizzi dobbiamo:\n",
    "\n",
    "1. Individuare le transazioni usate come input di ogni transazione.\n",
    "2. Individuare gli indirizzi di tali transazioni.\n",
    "3. Per ogni transazione, collegare il primo indirizzo della prima transazione di input con tutti\n",
    "   gli altri indirizzi delle altre transazioni di input.\n",
    "\n",
    "Utilizziamo il DataFrame `spent_df` creato in precedenza per il calcolo dell'UTXO poiché mette\n",
    "sulla stessa riga due transazioni:\n",
    "\n",
    "- Sulla colonna `tx` troviamo transazioni che riferiscono altre transazioni come loro input.\n",
    "- Sulla colonna `in_tx` troviamo transazioni riferite come input e di cui conosciamo l'address di\n",
    "  destinazione.\n",
    "\n",
    "L'euristica di **clustering** prevede inoltre che si considerino transazioni con più di un input,\n",
    "sarà necessario filtrare il dataset di conseguenza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df = pd.read_csv(\n",
    "    \"mappings.csv\",\n",
    "    names=[\"hash\", \"address\"],\n",
    "    dtype={\"hash\": str, \"address\": np.uint32},\n",
    "    nrows=n,\n",
    ")\n",
    "\n",
    "more_than_1 = spent_df.groupby(\"tx\")[\"in_tx\"].count().reset_index()\n",
    "more_than_1 = more_than_1[more_than_1[\"in_tx\"] > 1].rename(\n",
    "    columns={\"in_tx\": \"in_count\"}\n",
    ")\n",
    "more_than_1 = more_than_1.merge(spent_df, how=\"inner\", on=\"tx\").drop(\n",
    "    columns=[\"in_count\"]\n",
    ")\n",
    "\n",
    "more_than_1 = more_than_1.merge(mapping_df, how=\"inner\", on=\"address\")\n",
    "\n",
    "del spent_df\n",
    "del mapping_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora possiamo popolare il grafo andando a prendere per ogni transazione l'indirizzo del primo input\n",
    "per collegarlo agli indirizzi degli altri input della stessa transazione.\n",
    "\n",
    "Per evitare archi in loop sullo stesso nodo controlliamo che i due nodi siano differenti prima di\n",
    "aggiungere l'arco.\n",
    "\n",
    "Non è necessario controllare che un arco venga aggiunto più volte poiché, se ci provassimo, il\n",
    "risultato sarebbe in quanto il controllo viene già fatto internamente da `NetworkX`.\n",
    "\n",
    "Non ho aggiunto tutti gli address presenti nel dataset di mapping come nodi del grafo:\n",
    "\n",
    "- Per risparmiare memoria.\n",
    "- Molti sarebbero rimasti nodi singoli se non riferiti.\n",
    "\n",
    "Calcoliamo infine le componenti connesse ottenendo i cluster e ordiniamoli per dimensione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr_graph = nx.Graph()\n",
    "addresses = more_than_1.groupby(\"tx\")[\"hash\"].apply(list)\n",
    "for a in addresses:\n",
    "    for i in range(1, len(a)):\n",
    "        if a[0] != a[i]:\n",
    "            addr_graph.add_edge(a[0], a[i])\n",
    "\n",
    "clusters = sorted(nx.connected_components(addr_graph), key=len, reverse=True)\n",
    "\n",
    "del addr_graph\n",
    "del more_than_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi cluster\n",
    "\n",
    "Procediamo ora con qualche analisi sui cluster ottenuti come\n",
    "\n",
    "- Dimensione media, minima e massima dei cluster.\n",
    "- Distribuzione della dimensione dei cluster.\n",
    "\n",
    "Uno degli obbiettivi è quello di mettere in evidenza la distribuzione _power low_ della dimensione\n",
    "dei cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sizes = [len(cluster) for cluster in clusters]\n",
    "\n",
    "print(f\"Numero di cluster trovati: {len(clusters)}\")\n",
    "print(f\"Dimensione media: {np.mean(cluster_sizes):.4f}\")\n",
    "print(f\"Dimensione minima: {cluster_sizes[-1]}\")\n",
    "print(f\"Dimensione massima: {cluster_sizes[0]}\")\n",
    "print(f\"Dimensione dei 10 cluster più grandi: {cluster_sizes[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "\n",
    "Come plot ho optato per:\n",
    "\n",
    "- Un istogramma delle frequenze per ciascuna dimensione\n",
    "- Uno scatter plot in grado di evidenziare la distribuzione _power low_ che governa tali dimensioni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "ax1.hist(cluster_sizes, bins=20, ec=\"w\", color=\"darkred\")\n",
    "ax1.set_title(\"Distribuzione della dimensione dei cluster\")\n",
    "ax1.set_xlabel(\"Dimensione cluster\")\n",
    "ax1.set_ylabel(\"Frequenza della dimensione\")\n",
    "ax1.set_yscale(\"log\")\n",
    "\n",
    "cluster_sizes_count = {cs: 0 for cs in cluster_sizes}\n",
    "for cs in cluster_sizes:\n",
    "    cluster_sizes_count[cs] += 1\n",
    "\n",
    "data = list(zip(*[(k, cluster_sizes_count[k]) for k in cluster_sizes_count.keys()]))\n",
    "\n",
    "ax2.scatter(list(data[0]), list(data[1]), marker=\"x\", c=\"chocolate\")\n",
    "ax2.set_xlabel(\"Dimensione cluster\")\n",
    "ax2.set_ylabel(\"Frequenza della dimensione\")\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deanonimizzazione\n",
    "\n",
    "Passiamo alla parte di deanonimizzazione degli indirizzi considerando solo i 10 cluster più grandi.\n",
    "\n",
    "Per entrambi i siti ho utilizzato gli strumenti della libreria `BeautifulSoup` in quanto le pagine\n",
    "fornivano le informazioni rilevanti nel loro codice `html`.\n",
    "\n",
    "### WalletExplorer\n",
    "\n",
    "Ho iniziato dal sito [WalletExplorer](https://www.walletexplorer.com/) che si presenta con una form\n",
    "in cui è possibile inserire l'hash dell'indirizzo che cerchiamo.\n",
    "\n",
    "![wallet_explorer1_img](images/wallet_explorer1.png)\n",
    "\n",
    "Il `tag` che cerchiamo è un tag di input con il campo `name` equivalente a `q`. Possiamo dunque\n",
    "effettuare una richiesta `GET` in cui specifichiamo il parametro `params` tramite un dizionario\n",
    "di questo tipo\n",
    "\n",
    "```python\n",
    "{\"q\": address}\n",
    "```\n",
    "\n",
    "In questo modo otterremo una pagina simile a questa\n",
    "\n",
    "![wallet_explorer2_img](images/wallet_explorer2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.walletexplorer.com/\"\n",
    "session = requests.Session()\n",
    "session.headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    + \" (KHTML, like Gecko) Chrome/61.0.3163.100Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "def scrape_wallet_explorer(address: str) -> None | str:\n",
    "    r = session.get(base_url, params={\"q\": address})\n",
    "    if r.status_code == 200:\n",
    "        soup = bs4.BeautifulSoup(r.text, \"html.parser\")\n",
    "        wallet_tag = soup.find(\"h2\")\n",
    "        if isinstance(wallet_tag, bs4.Tag):\n",
    "            text = wallet_tag.get_text()\n",
    "            wallet_name = text.split(\" \")[1]\n",
    "            if wallet_name.startswith(\"[\"):\n",
    "                return None\n",
    "            else:\n",
    "                return wallet_name\n",
    "    else:\n",
    "        return f\"Errore {r.status_code}: {r.reason}\"\n",
    "\n",
    "\n",
    "too_many_requests = False\n",
    "for cluster_ID, cluster in enumerate(clusters[:10], start=1):\n",
    "    if too_many_requests == True:\n",
    "        break\n",
    "    for addr_ID, addr in enumerate(cluster):\n",
    "        if addr_ID < 10:\n",
    "            time.sleep(3)\n",
    "            wallet_name = scrape_wallet_explorer(addr)\n",
    "            if wallet_name != None:\n",
    "                if wallet_name.startswith(\"Errore\"):\n",
    "                    too_many_requests = True\n",
    "                print(f\"Cluster {cluster_ID}: {wallet_name}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Cluster {cluster_ID}: Anonimo\")\n",
    "            break\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'informazione che cerchiamo si trova nel primo tag `h2` che troviamo nella pagina.\n",
    "\n",
    "![wallet_explorer4](images/wallet_explorer4.png)\n",
    "\n",
    "Una volta agganciato il tag con `BeautifulSoup` possiamo direttamente ricavare il testo contenuto\n",
    "tramite la funzione `get_text()`.\n",
    "\n",
    "Se la stringa che ricaviamo è un numero racchiuso tramite parentesi quadre significa che non siamo\n",
    "riusciti a deanonimizzare l'indirizzo.\n",
    "\n",
    "Se invece troviamo una parola siamo riusciti a deanonimizzare l'indirizzo e possiamo passare al\n",
    "cluster successivo.\n",
    "\n",
    "#### AntiScraping\n",
    "\n",
    "Nel caso in cui si effetuino troppe richieste in poco tempo alla pagina questa ci blocca ritornando\n",
    "un codice di errore `429`.\n",
    "\n",
    "Per evitare di essere bloccato ho aggiunto un ritardo di 3 secondi tra una richiesta e l'altra.\n",
    "\n",
    "Nel caso si effettuassero però molte richieste il sito ci bloccherebbe anche con tempi di attesa\n",
    "più lunghi tra una richiesta e l'altra.\n",
    "\n",
    "### BitInfoCharts\n",
    "\n",
    "Per la seconda fase di scraping utilizziamo il sito [BitInfoCharts](https://bitinfocharts.com/)\n",
    "\n",
    "![bit_info_charts1](images/bit_info_charts2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://bitinfocharts.com/bitcoin/address/\"\n",
    "session = requests.Session()\n",
    "session.headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    + \" (KHTML, like Gecko) Chrome/61.0.3163.100Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "def scrape_bit_info_charts(address: str) -> None | str:\n",
    "    r = session.get(base_url + address)\n",
    "    if r.status_code == 200:\n",
    "        soup = bs4.BeautifulSoup(r.text, \"html.parser\")\n",
    "        small_tags = soup.find_all(\"small\")\n",
    "        for tag in small_tags:\n",
    "            if isinstance(tag, bs4.Tag):\n",
    "                text = tag.get_text()\n",
    "                if text.startswith(\"wallet:\"):\n",
    "                    return text.split(\" \")[1]\n",
    "\n",
    "        return None\n",
    "    else:\n",
    "        return f\"Errore {r.status_code}: {r.reason}\"\n",
    "\n",
    "\n",
    "http_error = False\n",
    "for cluster_ID, cluster in enumerate(clusters[:10], start=1):\n",
    "    if http_error == True:\n",
    "        break\n",
    "    for addr_ID, addr in enumerate(cluster):\n",
    "        if addr_ID < 10:\n",
    "            time.sleep(2)\n",
    "            wallet_name = scrape_bit_info_charts(addr)\n",
    "            if wallet_name != None and not wallet_name.isdigit():\n",
    "                if wallet_name.startswith(\"Errore\"):\n",
    "                    http_error = True\n",
    "                    print(wallet_name)\n",
    "                    break\n",
    "                print(f\"Cluster {cluster_ID}: {wallet_name}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Cluster {cluster_ID}: Anonimo\")\n",
    "            break\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
