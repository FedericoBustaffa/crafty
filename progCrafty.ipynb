{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRAFTY: Clustering and scRAping For biTcoin deanonYmization\n",
    "\n",
    "Il progetto è presentato sotto forma di notebook, integrando al codice una relazione che spiega i vari passi.\n",
    "\n",
    "Qui di seguito ci sono le librerie utilizzate con relativi `import`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lettura dei file .csv che compongono il dataset\n",
    "\n",
    "Inizialmente vengono importati i vari file `.csv` che fanno parte della sottocartella della directory corrente `craftyDataset/2013`. Oltre ad importarli, vengono già modificati il dataset `tx` trasformando tutte le date della colonna `ts` (timestamp) in date facilmente leggibili, e il dataset `mapping` utilizzando la colonna `addressId` come indice (servirà per facilitare la deanonimizzazione)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elenco delle transizioni\n",
    "tx = pd.read_csv(\n",
    "    \"transactions.csv\",\n",
    "    names=[\"ts\", \"blkID\", \"txId\", \"isCoinbase\", \"fee\"],\n",
    ")\n",
    "\n",
    "# converto il campo timestamp in una data leggibile\n",
    "tx[\"ts\"] = pd.to_datetime(tx[\"ts\"], unit=\"s\")\n",
    "\n",
    "# elenco degli inputs delle transizioni\n",
    "inp = pd.read_csv(\"inputs.csv\", names=[\"txId\", \"prevTxId\", \"prevTxPos\"])\n",
    "\n",
    "# elenco degli outputs\n",
    "out = pd.read_csv(\n",
    "    \"outputs.csv\",\n",
    "    names=[\"txId\", \"position\", \"addressId\", \"amount\", \"scriptType\"],\n",
    ")\n",
    "\n",
    "# file che mappa gli indirizzi unici con gli hash\n",
    "mapping = pd.read_csv(\"mappings.csv\", names=[\"hash\", \"addressId\"])\n",
    "mapping.set_index(\"addressId\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi sul dataset\n",
    "\n",
    "### 1) distribuzione del numero di transizioni per blocco (occupazione del blocco nell'intero periodo temporale considerato)\n",
    "\n",
    "Quest' analisi si basa sul raggruppare le transizioni sul campo `blkID` e contare quante transizioni ci sono per blocco mediante la funzione `size()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txDistr = tx.groupby(\"blkID\").size()  # numero di transazioni per blocco\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(\n",
    "    txDistr.index, txDistr.values, color=\"tab:orange\", linestyle=\"dashed\", linewidth=1\n",
    ")\n",
    "\n",
    "plt.xlabel(\"ID del blocco\")\n",
    "plt.ylabel(\"n. di transizioni\")\n",
    "plt.title(\"distribuzione del numero di transizioni per blocco\")\n",
    "plt.legend([\"transizioni\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) evoluzione dell'occupazione dei blocchi nel tempo considerando intervalli temporali di due mesi. In questo caso produrre un grafico che riporti il numero di transazioni medie per ogni periodo considerato.\n",
    "\n",
    "Creo una lista contenente le date di riferimento per intervalli di tempo lunghi 2 mesi, cosi da poter usare la funzione `between()` per includere tutte le transizioni effettuate in quell'intervallo (`twoMonthDate[i]` e `twoMonthDate[i+1]`). Le liste contengono le transazioni effettuate nel bimestre di ogni anno per tutti e 4 gli anni. Per il plot uso un grouped bar plot in modo da visualizzare per ogni anno tutti gli intervalli bimestrali.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoMonthsDate = pd.date_range(start=\"2009-01-01\", end=\"2013-01-01\", freq=\"2MS\")\n",
    "\n",
    "jan_feb = list()\n",
    "mar_apr = list()\n",
    "may_jun = list()\n",
    "jul_aug = list()\n",
    "sep_oct = list()\n",
    "nov_dec = list()\n",
    "\n",
    "for i in range(0, len(twoMonthsDate) - 1):\n",
    "    daysBetweenDate = tx[tx[\"ts\"].between(twoMonthsDate[i], twoMonthsDate[i + 1])]\n",
    "    meanDistr = daysBetweenDate.groupby([\"blkID\"]).size().mean()\n",
    "    if twoMonthsDate[i].month_name() == \"January\":\n",
    "        jan_feb.append(meanDistr)\n",
    "    elif twoMonthsDate[i].month_name() == \"March\":\n",
    "        mar_apr.append(meanDistr)\n",
    "    elif twoMonthsDate[i].month_name() == \"May\":\n",
    "        may_jun.append(meanDistr)\n",
    "    elif twoMonthsDate[i].month_name() == \"July\":\n",
    "        jul_aug.append(meanDistr)\n",
    "    elif twoMonthsDate[i].month_name() == \"September\":\n",
    "        sep_oct.append(meanDistr)\n",
    "    elif twoMonthsDate[i].month_name() == \"November\":\n",
    "        nov_dec.append(meanDistr)\n",
    "\n",
    "\n",
    "x = np.array([1, 2, 3, 4])\n",
    "width = 0.1\n",
    "\n",
    "\n",
    "bar1 = plt.bar(x - 2 * width, jan_feb, width, color=\"tab:red\")\n",
    "bar2 = plt.bar(x - 1 * width, mar_apr, width, color=\"tab:orange\")\n",
    "bar3 = plt.bar(x, may_jun, width, color=\"yellow\")\n",
    "bar4 = plt.bar(x + 1 * width, jul_aug, width, color=\"tab:cyan\")\n",
    "bar5 = plt.bar(x + 2 * width, sep_oct, width, color=\"tab:green\")\n",
    "bar6 = plt.bar(x + 3 * width, nov_dec, width, color=\"tab:olive\")\n",
    "plt.xticks(x + 0.5 * width, [2009, 2010, 2011, 2012])\n",
    "plt.xlabel(\"Anni\")\n",
    "plt.ylabel(\"Distribuzione delle transazioni per blocco\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend([\"Gen-Feb\", \"Mar-Apr\", \"Mag-Giu\", \"Lug-Ago\", \"Set-Ott\", \"Nov-Dic\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) ammontare totale degli UTXO al momento dell'ultima transizione registrata nella blockchain considerata\n",
    "\n",
    "Ogni transizione può contentere più input/output, quindi per calcolare la somma degli UTXO devo tener traccia della posizione di ogni output per fare una `merge()` e ottenere gli output spesi. Per ottenere la somma degli UTXO sottraggo al totale degli `amount` degli output gli `amount` degli output spesi (contenuti nel merge fatto in precedenza)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txSpent = out.merge(\n",
    "    inp, right_on=[\"prevTxId\", \"prevTxPos\"], left_on=[\"txId\", \"position\"]\n",
    ")\n",
    "\n",
    "utxo = out[\"amount\"].sum() - txSpent[\"amount\"].sum()\n",
    "print(f\"UTXO: {utxo} BTC\")\n",
    "\n",
    "explode = (0.8, 0)\n",
    "plt.pie(\n",
    "    [utxo, txSpent[\"amount\"].sum()],\n",
    "    explode=explode,\n",
    "    labels=[\"UXTO\", \"totale speso\"],\n",
    "    autopct=\"%.2f%%\",\n",
    "    shadow=\"True\",\n",
    "    startangle=45,\n",
    ")\n",
    "plt.axis(\"equal\")\n",
    "plt.title(\"Distribuzione tra UTXO e ammontare speso\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) distribuzione degli intervalli di tempo che intercorrono tra la transazione che genera un output e quella che lo consuma, per gli output spesi nel periodo considerato\n",
    "\n",
    "Inizio con il fare due merge, utili a capire quando il valore preso in input di una transizione viene generato e quando viene speso. Il primo viene ottenuto mediante una merge tra `output` e `tx`, l'altro su `input` e sempre `tx`. Si fa un terzo merge tra le due generate per ottenere la differenza di tempo in giorni tra `spentDate` e `creationDate` per ciascuna transizione, eliminando alcune colonne che potrebbero essere `Null` con la funzione `dropna()` (succede nel caso degli output non spesi).\n",
    "Infine si raggruppa in base al numero di giorni di differenza e si calcolano le transazioni che hanno tale differenza di giorni. Si crea un nuovo dataset che contiene tali informazioni e si plotta utilizzando una scala logaritmica per quanto riguarda il numero di transizioni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spentTX = inp.merge(tx, on=\"txId\").rename(columns={\"ts\": \"spentDate\"})[\n",
    "    [\"prevTxId\", \"prevTxPos\", \"spentDate\"]\n",
    "]\n",
    "createdTX = out.merge(tx, on=\"txId\").rename(columns={\"ts\": \"creationDate\"})[\n",
    "    [\"txId\", \"position\", \"blkID\", \"creationDate\"]\n",
    "]\n",
    "\n",
    "timeRes = createdTX.merge(\n",
    "    spentTX,\n",
    "    how=\"left\",\n",
    "    left_on=[\"txId\", \"position\"],\n",
    "    right_on=[\"prevTxId\", \"prevTxPos\"],\n",
    ")[[\"txId\", \"blkID\", \"creationDate\", \"spentDate\"]]\n",
    "timeRes = timeRes.dropna()\n",
    "timeRes[\"diffDays\"] = (timeRes[\"spentDate\"] - timeRes[\"creationDate\"]).dt.days\n",
    "\n",
    "groupByDiff = timeRes.groupby(\"diffDays\")[\"txId\"].size()\n",
    "days = pd.Series(list(groupByDiff.index))\n",
    "numberOfOut = pd.Series(list(groupByDiff.values))\n",
    "\n",
    "timeDistr = pd.concat([numberOfOut, days], axis=\"columns\", keys=[\"outputs\", \"days\"])\n",
    "\n",
    "plt.stackplot(timeDistr[\"days\"], timeDistr[\"outputs\"], color=\"c\")\n",
    "plt.xlabel(\"giorni di differenza\")\n",
    "plt.ylabel(\"transazioni\")\n",
    "plt.yscale(value=\"log\")\n",
    "\n",
    "plt.title(\"distribuzione degli intervalli di tempo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuzione fee nulle e positive per ogni transazione, raggruppate per anno\n",
    "\n",
    "Ricavo le transazioni che hanno `fee` positive e quelle che hanno `fee` uguale a 0. Vengono raggruppate per anno e calcolo il numero delle rispettive transazioni che le contengono per anno. Plottando in questo modo vedo l'andamento tra le due tipologie di fee, avendo un'idea di quante hanno `fee` positive e quante nulle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posFee = tx.loc[tx[\"fee\"] > 0]\n",
    "nullFee = tx.loc[tx[\"fee\"] == 0]\n",
    "\n",
    "posFeeGrouped = posFee.groupby(posFee[\"ts\"].dt.year).size()\n",
    "print(f\"Fee positive per anno: {posFeeGrouped}\\n\")\n",
    "nullFeeGrouped = nullFee.groupby(nullFee[\"ts\"].dt.year).size()\n",
    "print(f\"Fee nulle per anno: {nullFeeGrouped}\\n\")\n",
    "\n",
    "\n",
    "plt.title(\"Distribuzione fee nulle e positive\")\n",
    "plt.xlabel(\"anni\")\n",
    "plt.ylabel(\"transazioni contenenti fee\")\n",
    "plt.xticks([2009, 2010, 2011, 2012])\n",
    "plt.stackplot(\n",
    "    [2009, 2010, 2011, 2012],\n",
    "    nullFeeGrouped,\n",
    "    posFeeGrouped,\n",
    "    colors=[\"tab:olive\", \"tab:green\"],\n",
    ")\n",
    "plt.legend([\"fee nulle\", \"fee positive\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterizzazione degli indirizzi di Bitcoin\n",
    "\n",
    "Abbiamo bisogno di un dataset che contenga l'`addressId` delle transazioni generate, ottenibile\n",
    "facendo un merge tra input e output (`mergeInpOutTX`).\n",
    "\n",
    "Calcolo il numero di input per ciascuna transizione per eliminare le transizioni che hanno 1 o 0\n",
    "input, non utili alla nostra analisi.\n",
    "\n",
    "In `addressTX` ho una lista che ha come \"indice\" l'ID della transizione, e come valore una lista\n",
    "di tutti gli indirizzi degli input.\n",
    "\n",
    "A questo punto, l'algoritmo di `address clustering` consiste nel:\n",
    "\n",
    "- inserire tutti i nodi, ovvero gli indirizzi che sono all'interno del file mapping\n",
    "- per ogni lista contenuta in addressTX\n",
    "  - inserisco il primo elemento della lista all'interno del grafo e lo collego a tutti gli altri\n",
    "    nodi presenti nella stessa lista (solo se non è già presente)\n",
    "- si calcolano le componenti connesse con la funzione `nx.connected_component()`\n",
    "\n",
    "Si prendono i cluster con il numero di indirizzi maggiori, li trasformo da indirizzi unici ad\n",
    "indirizzi hash e trovo alcune informazioni che possono servire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphAddressTX = nx.Graph()\n",
    "\n",
    "mergeInpOutTX = inp.merge(\n",
    "    out, left_on=[\"prevTxId\", \"prevTxPos\"], right_on=[\"txId\", \"position\"]\n",
    ").rename(columns={\"txId_x\": \"txId\"})[[\"txId\", \"prevTxId\", \"addressId\"]]\n",
    "nOfInputsTX = mergeInpOutTX.groupby(\"txId\")[\"prevTxId\"].size()\n",
    "\n",
    "idGreater1 = np.greater(nOfInputsTX.values, 1)  # boolean\n",
    "mergeInpOutTX = mergeInpOutTX.loc[\n",
    "    mergeInpOutTX[\"txId\"].isin((nOfInputsTX.loc[idGreater1]).index)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "addressTX = mergeInpOutTX.groupby(\"txId\")[\"addressId\"].apply(list)\n",
    "\n",
    "graphAddressTX.add_nodes_from(mapping.index)  # step 1\n",
    "\n",
    "for listAddr in addressTX.values:  # step 2\n",
    "    firstAddr = listAddr[0]\n",
    "    for addr in listAddr[1:]:\n",
    "        if firstAddr != addr and not (graphAddressTX.has_edge(firstAddr, addr)):\n",
    "            graphAddressTX.add_edge(firstAddr, addr)\n",
    "\n",
    "compGenerator = nx.connected_components(graphAddressTX)  # step 3\n",
    "clusters = list(compGenerator)\n",
    "sortedClst = sorted(clusters, key=len, reverse=True)\n",
    "lenClst = [len(cluster) for cluster in sortedClst]\n",
    "\n",
    "greater10Clst = sortedClst[:10]\n",
    "greater10ClstSize = lenClst[:10]\n",
    "\n",
    "# servirà per lo scraping\n",
    "greater10ClstHash = [\n",
    "    (mapping.loc[address][\"hash\"] for address in cluster) for cluster in greater10Clst\n",
    "]\n",
    "\n",
    "print(f\"Numero di cluster: {len(sortedClst)}\\n\")\n",
    "print(f\"Dimensione media dei cluster: {sum(lenClst) / len(lenClst)}\\n\")\n",
    "print(f\"Dimensione minima: {lenClst[-1]}\\n\")\n",
    "print(f\"Dimensione massima: {lenClst[0]}\\n\")\n",
    "print(f\"Dimensione dei 10 cluster più grandi: {greater10ClstSize}\\n\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Distribuzione dimensione dei cluster\")\n",
    "plt.xlabel(\"clusters\")\n",
    "plt.ylabel(\"n. di indirizzi\")\n",
    "plt.plot(lenClst, color=\"tab:blue\")\n",
    "plt.yscale(value=\"log\")\n",
    "plt.xscale(value=\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "In questa parte l'obiettivo è quello di riuscire a deanonimizzare più indirizzi possibili per\n",
    "associare ad ogni cluster il relativo wallet a cui fa riferimento.\n",
    "\n",
    "Per deanonimizzare vengono usati i due siti `Wallet Explorer` e `Bitcoin Info Charts`, ai quali mi\n",
    "relaziono in modo differente.\n",
    "\n",
    "Non cerco l'associazione per tutti gli indirizzi del cluster, ma ho un numero massimo di iterazioni,\n",
    "in più appena trovo il primo indirizzo esco e passo al cluster successivo.\n",
    "\n",
    "Viene fatto questo sia perchè ci metterebbe troppo tempo che per il fatto che il sito potrebbe\n",
    "bloccare il nostro IP per il numero di richieste fatte e non deanonimizzare tutti i cluster.\n",
    "\n",
    "Questo lo possiamo capire utilizzando lo status code: nel caso fosse `429` il sito ci avviserà che\n",
    "stanno arrivando troppe richieste, non ottenendo più risposte.\n",
    "\n",
    "### Wallet Explorer\n",
    "\n",
    "Questo sito, oltre a deanonimizzare l'indirizzo, restituisce il numero di transazioni effettuate da\n",
    "quell'indirizzo e il numero di indirizzi ad esso collegati.\n",
    "\n",
    "Osservando la pagina HTML il nome del wallet associato all'indirizzo è presente nel tag `h2`.\n",
    "\n",
    "Inizialmente quindi, per accedere alla pagina facciamo la request con la funzione `get()`\n",
    "aggiungendo la query con parametro `q = addr`.\n",
    "\n",
    "Utilizzando `BeautifulSoap`, attraverso la funzione `find()` possiamo accedere al primo tag `h2`\n",
    "presente nella pagina.\n",
    "\n",
    "La stringa che ci interessa si ottiene accedendo al campo `text`, prendendo in considerazione solo\n",
    "il nome del wallet utilizzando la `split()`.\n",
    "\n",
    "Una volta trovato, lo associamo all'interno di un oggetto contenente tutti i nomi dei 10 cluster\n",
    "più grandi.\n",
    "\n",
    "Quando non si riesce a deanonimizzare un indirizzo, al posto del nome del wallet vi è una sequenza\n",
    "di numeri all'interno di parentesi quadre e nell'oggetto come nome del cluster rimarrà\n",
    "`'Unknown wallet name'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wallet Explorer\n",
    "\n",
    "headerAgents = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    + \" (KHTML, like Gecko) Chrome/61.0.3163.100Safari/537.36\"\n",
    "}\n",
    "wexUrl = \"https://www.walletexplorer.com/\"\n",
    "\n",
    "\n",
    "def scraperWallet(cluster, walletDict, value):\n",
    "    nIteration = 15\n",
    "    clstFound = False\n",
    "    for addrCount, addr in enumerate(cluster, start=1):\n",
    "        time.sleep(3)\n",
    "        if addrCount > nIteration or clstFound:\n",
    "            break\n",
    "        walletDict[f\"{value} cluster\"] = \"Unknown wallet name\"\n",
    "\n",
    "        request = requests.get(wexUrl, {\"q\": addr}, headers=headerAgents)\n",
    "        if request.status_code == 429:\n",
    "            print(\"Troppe richieste...\")\n",
    "            break\n",
    "        html = request.text\n",
    "        bs = BeautifulSoup(html, \"html.parser\")\n",
    "        walletName = bs.find(\"h2\")\n",
    "\n",
    "        if walletName is not None:\n",
    "            walletName = walletName.text.split(\" \")[1]\n",
    "            if walletName[0] != \"[\":\n",
    "                clstFound = True\n",
    "    if clstFound:\n",
    "        walletDict[f\"{value} cluster\"] = walletName\n",
    "\n",
    "\n",
    "walletDict = {}\n",
    "clstNum = 1\n",
    "for clst in greater10ClstHash:\n",
    "    scraperWallet(clst, walletDict, clstNum)\n",
    "    clstNum += 1\n",
    "\n",
    "print(walletDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitcoin Info Charts\n",
    "\n",
    "Nel caso di Bitcoin Info Charts, la pagina di interesse non contiene solo la query, ma bisogna\n",
    "concatenare tra la home del sito e l'indirizzo che vogliamo cercare la stringa `bitcoin/address/`.\n",
    "\n",
    "Inoltre, non userò nuovamente `BeautifulSoup` ma `Selenium`, con il quale automatizzerò il processo\n",
    "di ricerca mediante il driver del browser (in questo caso Chrome).\n",
    "\n",
    "Sempre per evitare di essere bloccati e respinti dal sito, utilizzo delle `sleep()` e limito il\n",
    "numero di indirizzi.\n",
    "\n",
    "In questa pagina html, il nome del wallet è contenuto all'interno del tag `small`. Cerco il tag che\n",
    "contiene il testo con il nome, controllo se non è un valore che contiene solo cifre, lo estraggo e\n",
    "lo assegno al cluster sempre all'interno di un oggetto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bitcoin Info Charts\n",
    "\n",
    "btcUrl = \"https://bitinfocharts.com/\"\n",
    "\n",
    "\n",
    "def scraperBTCinfoCharts(cluster, BTCDict, value):\n",
    "    nIteration = 10\n",
    "    clstFound = False\n",
    "    for addrCount, addr in enumerate(cluster, start=1):\n",
    "        time.sleep(3)\n",
    "        if addrCount > nIteration or clstFound:\n",
    "            break\n",
    "\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(btcUrl + \"bitcoin/address/\" + addr)\n",
    "\n",
    "        for smallTag in driver.find_elements(By.TAG_NAME, \"small\"):\n",
    "            if \":\" not in smallTag.text:\n",
    "                continue\n",
    "\n",
    "            walletName = smallTag.text.split(\": \")[1]\n",
    "            if not walletName.isdigit():\n",
    "                clstFound = True\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "    if clstFound:\n",
    "        BTCDict[f\"{value} cluster\"] = walletName\n",
    "    else:\n",
    "        BTCDict[f\"{value} cluster\"] = \"Unknown wallet name\"\n",
    "\n",
    "\n",
    "clstNum = 1\n",
    "btcDict = {}\n",
    "for clst in greater10ClstHash:\n",
    "    scraperBTCinfoCharts(clst, btcDict, clstNum)\n",
    "    clstNum += 1\n",
    "\n",
    "\n",
    "print(btcDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusioni\n",
    "\n",
    "Con le due analisi abbiamo due deanonimizzazioni differenti, date probabilmente da algoritmi diversi usati dai due siti. Da `Wallet Explorer` abbiamo i seguenti cluster:\n",
    "\n",
    "- cluster 1 -> CoinJoinMess\n",
    "- cluster 2 -> SilkRoadMarketplace\n",
    "- cluster 3 -> Unknown wallet name\n",
    "- cluster 4 -> Instawallet.org\n",
    "- cluster 5 -> Unknown wallet name\n",
    "- cluster 6 -> BTC-e.com-old\n",
    "- cluster 7 -> BtcDice.com\n",
    "- cluster 8 -> Unknown wallet name\n",
    "- cluster 9 -> Unknown wallet name\n",
    "- cluster 10 -> Unknown wallet name\n",
    "\n",
    "Per `Bitcoin Info Charts` invece abbiamo i seguenti:\n",
    "\n",
    "- cluster 1 -> F2Pool\n",
    "- cluster 2 -> SilkRoadMarketplace\n",
    "- cluster 3 -> Unknown wallet name\n",
    "- cluster 4 -> Instawallet.org\n",
    "- cluster 5 -> Unknown wallet name\n",
    "- cluster 6 -> Eligius\n",
    "- cluster 7 -> Unknown wallet name\n",
    "- cluster 8 -> Unknown wallet name\n",
    "- cluster 9 -> Unknown wallet name\n",
    "- cluster 10 -> Unknown wallet name\n",
    "\n",
    "Abbiamo che i cluster 1 e 6 sono de-anonimizzati in modo diverso, probabilmente per euristica usata o mancato aggiornamento del sito con indirizzi più recenti, mentre il cluster 7 su `Bitcoin Info Charts` non è stato possibile de-anonimizzarlo, probabilmente per il numero di tentativi bassi.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
