{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRAFTY\n",
    "\n",
    "# Clustering and scRAping For biTcoin deanonYmization\n",
    "\n",
    "## Dataset e gestione delle memoria\n",
    "\n",
    "Importiamo i dataset con `pandas` cercando di risparmiare memoria andando a specificare,\n",
    "tramite il parametro `dtype` il tipo e la dimensione dei dati di ogni colonna.\n",
    "\n",
    "In questo modo risparmiamo quasi 1GB di RAM e velocizziamo il caricamento del dataset. Sono\n",
    "stato in parte costretto a procedere in questo modo poiché non sempre riuscivo a contenere\n",
    "tutti i dati generati nella RAM della macchina (8GB di cui metà occupati da processi di\n",
    "sistema).\n",
    "\n",
    "Più avanti andrò inoltre a deallocare i DataFrame più grandi una volta che questi non sono\n",
    "più necessari. Questo potrebbe creare qualche problema nel caso si voglia eseguire tutto il\n",
    "notebook e in seguito eseguire qualche cella specifica poiché si potrebbe far riferimento a\n",
    "dati già deallocati.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import networkx as nx\n",
    "import bs4\n",
    "import requests\n",
    "import time\n",
    "\n",
    "tx_df = pd.read_csv(\n",
    "    \"transactions.csv\",\n",
    "    names=[\"timestamp\", \"blk\", \"tx\", \"is_coinbase\", \"fee\"],\n",
    "    dtype={\n",
    "        \"timestamp\": np.uint32,\n",
    "        \"blk\": np.uint32,\n",
    "        \"tx\": np.uint32,\n",
    "        \"is_coinbase\": np.uint8,\n",
    "        \"fee\": np.uint64,\n",
    "    },\n",
    ")\n",
    "tx_df[\"timestamp\"] = pd.to_datetime(tx_df[\"timestamp\"], unit=\"s\")\n",
    "tx_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi\n",
    "\n",
    "In questa prima fase andiamo ad effettuare delle analisi generali sul dataset.\n",
    "\n",
    "### Distribuzione del numero di transazioni per blocco\n",
    "\n",
    "Vogliamo ricavare la **distribuzione** del numero di transazioni per blocco, nell'intero\n",
    "periodo temporale considerato.\n",
    "\n",
    "Per prima cosa raggruppiamo il dataset per blocco.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "blk_grp = tx_df.groupby(\"blk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dato che ogni riga del dataset rappresenta una transazione e dato che ogni transazione ha un\n",
    "identificatore univoco, questa comparirà al più una volta nel dataset.\n",
    "\n",
    "Siamo quindi in grado di ottenere il numero di transazioni per blocco tramite la funzione di\n",
    "aggregazione `count`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tx_per_blk = blk_grp[\"tx\"].count()\n",
    "tx_per_blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "\n",
    "Come sistema di plotting per la distribuzione del numero di transazioni per blocco abbiamo\n",
    "due punti di vista:\n",
    "\n",
    "- Istogramma delle frequenze di ogni dimensione dei blocchi trovata.\n",
    "- Distribuzione della dimensione dei blocchi nel tempo con un grafico a barre approssimato.\n",
    "\n",
    "Per plottare la seconda distribuzione ho optato per un grafico a barre ma plottare una barra\n",
    "per ogni blocco (quasi 200000 blocchi) sarebbe stato troppo lungo.\n",
    "\n",
    "Ho quindi deciso di raggruppare il dataset ulteriormente. Dato che volevo 30 barre, avevo\n",
    "bisogno di raggruppare ogni $n$ righe. Dove $n$ viene calcolato dividendo la lunghezza del\n",
    "dataset per 30.\n",
    "\n",
    "Ho infine calcolato la media di ogni gruppo ottenuto per avere una rappresentazione del\n",
    "numero di transazioni\n",
    "per blocco.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "ax1.hist(tx_per_blk, bins=20, ec=\"w\")\n",
    "ax1.set_title(\"Distribuzione delle transazioni per blocco\")\n",
    "ax1.set_xlabel(\"Dimensione del blocco\")\n",
    "ax1.set_ylabel(\"Quantità di blocchi\")\n",
    "ax1.set_yscale(\"log\")\n",
    "\n",
    "bins = 30\n",
    "group_size = len(tx_per_blk) // bins\n",
    "tx_per_blk_approx = tx_per_blk.groupby(np.arange(len(tx_per_blk)) // group_size).mean()\n",
    "\n",
    "ax2.bar(\n",
    "    tx_per_blk_approx.index,\n",
    "    tx_per_blk_approx,\n",
    "    color=\"tab:orange\",\n",
    "    label=\"Transazioni\",\n",
    ")\n",
    "ax2.set_title(\"Distribuzione delle transazioni per blocco nel tempo\")\n",
    "ax2.set_xlabel(\"Tempo\")\n",
    "ax2.set_ylabel(\"Transazioni per blocco\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occupazione dei blocchi nel tempo\n",
    "\n",
    "La seconda analisi prevede lo studio di come l'occupazione dei blocchi si è evoluta nel\n",
    "tempo. In particolare dobbiamo andare a considerare intervalli di due mesi.\n",
    "\n",
    "Dato che avevamo già raggruppato il dataset per blocchi, ho deciso di considerare come data\n",
    "di riferimento, la data della transazione meno recente come data di riferimento di ogni\n",
    "blocco.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transazioni per blocco con timestamp\n",
    "tx_per_blk = pd.concat([tx_per_blk, blk_grp[\"timestamp\"].min()], axis=\"columns\")\n",
    "tx_per_blk = tx_per_blk.rename(columns={\"tx\": \"tx_count\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ho infine utilizzato l'oggetto `Grouper` di Pandas per riuscire ad interagire comodamente\n",
    "con le date ed effettuare un raggruppamento bimestrale dei blocchi.\n",
    "\n",
    "Per ogni gruppo ho infine calcolato l'occupazione media dei blocchi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_per_blk = (\n",
    "    tx_per_blk.groupby(pd.Grouper(key=\"timestamp\", freq=\"2MS\"))[\"tx_count\"]\n",
    "    .mean()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "tx_per_blk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "\n",
    "Come metodologia di plotting ho optato per un semplice plot in grado di fornire una\n",
    "rappresentazione accurata dell'evoluzione dell'occupazione media dei blocchi ogni\n",
    "due mesi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(\n",
    "    tx_per_blk[\"timestamp\"],\n",
    "    tx_per_blk[\"tx_count\"],\n",
    "    c=\"tab:red\",\n",
    "    marker=\"o\",\n",
    "    mec=\"k\",\n",
    "    label=\"transazioni medie per blocco\",\n",
    ")\n",
    "\n",
    "plt.title(\"Occupazione media dei blocchi ogni 2 mesi\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Numero medio di transazioni per blocco\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "del tx_per_blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Totale UTXO\n",
    "\n",
    "Passiamo ora a calcolare l'ammontare totale degli **UTXO** al momento dell'ultima transazione\n",
    "registrata nel dataset.\n",
    "\n",
    "L'idea è quella di calcolare prima tutto l'output che è stato speso e poi sottrarlo  \n",
    "dall'output totale prodotto.\n",
    "\n",
    "Per arrivare a questo risultato effettuiamo un'operazione di `merge` tra il dataset degli\n",
    "**input** e il dataset degli **output**.\n",
    "\n",
    "Dato che non abbiamo identificatori univoci per ogni output ma solo per le transazioni, per\n",
    "calcolare l'UTXO totale dobbiamo:\n",
    "\n",
    "- Considerare le transazioni mai riferite come input da altre transazioni.\n",
    "- Considerare gli output mai riferiti da altre transazioni che però riferiscono altri output\n",
    "  all'interno della stessa transazione.\n",
    "\n",
    "Ho deciso quindi di filtrare il dataset degli output eliminando\n",
    "\n",
    "Ho deciso quindi di implementare l'operazione di `merge` (in questo caso di tipo _inner_)\n",
    "basandomi sia sulle transazioni che sugli output riferiti in input.\n",
    "\n",
    "Il risultato sarà un DataFrame `spent_df` in cui ogni riga è identificata univocamente dalla\n",
    "coppia `(in_tx, out_pos)`.\n",
    "\n",
    "In questo modo otterremo un dataset in cui saranno presenti solo le transazioni riferite\n",
    "come input da altre transazioni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df = pd.read_csv(\n",
    "    \"inputs.csv\",\n",
    "    names=[\"tx\", \"utxo_tx\", \"utxo_out_pos\"],\n",
    "    dtype={\"tx\": np.uint32, \"utxo_tx\": np.uint32, \"out_pos\": np.uint16},\n",
    ")\n",
    "\n",
    "out_df = pd.read_csv(\n",
    "    \"outputs.csv\",\n",
    "    names=[\"tx\", \"out_pos\", \"address\", \"amount\", \"script_type\"],\n",
    "    dtype={\n",
    "        \"tx\": np.uint32,\n",
    "        \"out_pos\": np.uint16,\n",
    "        \"address\": np.uint32,\n",
    "        \"amount\": np.uint64,\n",
    "        \"script_type\": np.uint8,\n",
    "    },\n",
    ")\n",
    "\n",
    "spent_df = (\n",
    "    out_df.merge(\n",
    "        in_df,\n",
    "        how=\"inner\",\n",
    "        left_on=[\"tx\", \"out_pos\"],\n",
    "        right_on=[\"utxo_tx\", \"utxo_out_pos\"],\n",
    "    )\n",
    "    .rename(columns={\"tx_x\": \"in_tx\", \"tx_y\": \"tx\"})\n",
    "    .drop(columns=[\"utxo_tx\", \"utxo_out_pos\", \"script_type\"])\n",
    ")\n",
    "\n",
    "del in_df\n",
    "\n",
    "print(spent_df.info())\n",
    "spent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto possiamo sommare i valori della colonna `amount` del DataFrame `spent_df`\n",
    "per ricavare l'output speso.\n",
    "\n",
    "Se invece sommiamo i valori della colonna `amount` del dataset degli **output** generati in\n",
    "tutto il periodo di tempo considerato ricaviamo il valore totale degli output sia speso che\n",
    "non speso.\n",
    "\n",
    "Calcolando la differenza tra il il valore totale degli output e il valore totale degli\n",
    "output spesi ricaviamo l'**UTXO** totale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utxo = out_df[\"amount\"].sum() - spent_df[\"amount\"].sum()\n",
    "print(f\"UTXO totale: {utxo}\")\n",
    "\n",
    "del out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "\n",
    "Tramite un grafico a torta è possibile comparare l'output speso con l'**UTXO**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.pie(\n",
    "    [utxo, spent_df[\"amount\"].sum()],\n",
    "    explode=(0.1, 0),\n",
    "    labels=[\"UTXO\", \"Totale speso\"],\n",
    "    colors=[\"tab:green\", \"tab:red\"],\n",
    "    autopct=\"%.2f%%\",\n",
    ")\n",
    "\n",
    "plt.title(\"UTXO\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuzione intervalli di tempo per il consumo di un output\n",
    "\n",
    "Abbiamo già a disposizione un DataFrame che mette in correlazione gli input e gli output.\n",
    "\n",
    "Per capire dopo quanto tempo è stato speso un output dobbiamo aggiungere il valore di\n",
    "`timestamp` sia per il valore in input che per il valore in output.\n",
    "\n",
    "Per riuscire a farlo ho effettuato un doppio `merge` con il DataFrame delle transazioni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spent_time_df = (\n",
    "    spent_df.merge(tx_df, how=\"inner\", on=\"tx\")\n",
    "    .drop(columns=[\"blk\", \"tx\", \"is_coinbase\", \"fee\"])\n",
    "    .rename(columns={\"timestamp\": \"spent_time\"})\n",
    ")\n",
    "\n",
    "spent_time_df = (\n",
    "    spent_time_df.merge(tx_df, how=\"inner\", left_on=\"in_tx\", right_on=\"tx\")\n",
    "    .drop(columns=[\"blk\", \"tx\", \"is_coinbase\", \"fee\"])\n",
    "    .rename(columns={\"timestamp\": \"gen_time\"})\n",
    ")\n",
    "\n",
    "print(spent_df.info())\n",
    "spent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ottenere gli intervalli possiamo effettuare la differenza tra il momento della creazione\n",
    "di un output e il momento in cui questo viene speso, ossia quando viene riferito come input\n",
    "da un transazione successiva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = (spent_time_df[\"spent_time\"] - spent_time_df[\"gen_time\"]).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "\n",
    "A questo punto abbiamo una Series `days`, contenente il numero di giorni di vita di ogni\n",
    "transazione.\n",
    "\n",
    "Sull'asse $x$ abbiamo la lunghezza degli intervalli di tempo (in giorni) prima che un output\n",
    "venisse speso.\n",
    "\n",
    "Sull'asse $y$ abbiamo invece la quantità di output che vengono spesi ogni $x$ giorni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(days, bins=40, ec=\"w\", color=\"teal\")\n",
    "\n",
    "plt.title(\"Distribuzione degli intervalli di tempo\")\n",
    "plt.xlabel(\"Giorni prima di spendere l'output\")\n",
    "plt.ylabel(\"Quantità di output\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuzione delle fee\n",
    "\n",
    "In quest'ultima fase vogliamo effettuare due analisi:\n",
    "\n",
    "1. Mettere a confronto la distribuzione di _fee_ nulle e la distribuzione di _fee_ non nulle\n",
    "   nel tempo.\n",
    "2. Vedere se esiste una correlazione tra il numero e il valor medio mensile di _fee_ non\n",
    "   nulle.\n",
    "\n",
    "Dato che le transazioni coinbase non hanno mai _fee_ non le consideriamo in quanto siamo\n",
    "interessati solo alle transazioni che potrebbero potenzialmente avere _fee_ non nulle.\n",
    "\n",
    "#### Distribuzione delle _fee_ nulle e non nulle a confronto\n",
    "\n",
    "In questa prima fase vogliamo vedere come il numero di fee nulle e non nulle è cambiato mese\n",
    "per mese.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_df = tx_df[tx_df[\"is_coinbase\"] == 0]\n",
    "\n",
    "zero_fee = tx_df[tx_df[\"fee\"] == 0]\n",
    "non_zero_fee = tx_df[tx_df[\"fee\"] > 0]\n",
    "\n",
    "zero_count = (\n",
    "    zero_fee.groupby(pd.Grouper(key=\"timestamp\", freq=\"1MS\"))[\"fee\"]\n",
    "    .count()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "non_zero_count = (\n",
    "    non_zero_fee.groupby(pd.Grouper(key=\"timestamp\", freq=\"1MS\"))[\"fee\"]\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "del tx_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot\n",
    "\n",
    "Il tipo di grafico è un grouped bar plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = datetime.timedelta(days=10)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "ax.bar(\n",
    "    pd.to_datetime(zero_count[\"timestamp\"]) - interval / 2,\n",
    "    zero_count[\"fee\"],\n",
    "    width=interval,\n",
    "    label=\"Fee nulle\",\n",
    ")\n",
    "\n",
    "ax.bar(\n",
    "    non_zero_count[\"timestamp\"] + interval / 2,\n",
    "    non_zero_count[\"fee\"],\n",
    "    width=interval,\n",
    "    label=\"Fee non nulle\",\n",
    ")\n",
    "\n",
    "ax.set_title(\"Fee nulle e non nulle a confronto\")\n",
    "ax.set_xlabel(\"Mese\")\n",
    "ax.set_ylabel(\"Quantità di fee\")\n",
    "fig.autofmt_xdate()\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlazione tra numero di _fee_ e il loro valore\n",
    "\n",
    "Vogliamo ora vedere se c'è una qualche correlazione tra il numero mensile di _fee_ non nulle\n",
    "e il valor medio mensile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_mean_value = (\n",
    "    non_zero_fee.groupby(pd.Grouper(key=\"timestamp\", freq=\"1MS\"))[\"fee\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "fee_df = non_zero_count.merge(non_zero_mean_value, how=\"inner\", on=\"timestamp\")\n",
    "\n",
    "print(fee_df.info())\n",
    "fee_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "\n",
    "Dato che abbiamo calcolato due distribuzioni ho optato per effettuare un doppio plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "ax.plot(\n",
    "    fee_df[\"timestamp\"],\n",
    "    fee_df[\"fee_x\"],\n",
    "    \"o-\",\n",
    "    mec=\"k\",\n",
    "    label=\"quantità mensile delle fee\",\n",
    "    c=\"seagreen\",\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    fee_df[\"timestamp\"],\n",
    "    fee_df[\"fee_y\"],\n",
    "    \"o-\",\n",
    "    mec=\"k\",\n",
    "    label=\"valor medio mensile delle fee\",\n",
    "    c=\"tab:orange\",\n",
    ")\n",
    "\n",
    "ax.set_title(\"Distribuzione quantità e valor medio delle fee delle nulle\")\n",
    "ax.set_xlabel(\"Data\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del fee_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come possiamo vedere dai grafici il numero di _fee_ non nulle tende a crescere nel tempo\n",
    "mentre il loro valor medio tende a diminuire.\n",
    "\n",
    "Per riuscire a comprendere meglio questo trend si dovrebbe mettere in correlazione anche il  \n",
    "numero di transazioni mensili.\n",
    "\n",
    "Ipotizzo che con il crescere della popolarità di bitcoin, gli utenti abbiano anche iniziato a\n",
    "partecipare in maniera più attiva.\n",
    "\n",
    "Ad esempio ricompensando, anche se con delle _fee_ basse, i miner che permettono di creare\n",
    "nuovi blocchi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterizzazione degli indirizzi\n",
    "\n",
    "In questa fase vogliamo implementare l'**euristica multi-input** per la clusterizzazione\n",
    "degli indirizzi.\n",
    "\n",
    "L'obbiettivo è quello di riunire in cluster, gli indirizzi appartenenti ad uno stesso utente.\n",
    "\n",
    "### Grafo degli indirizzi\n",
    "\n",
    "Per costruire il grafo degli indirizzi dobbiamo:\n",
    "\n",
    "1. Individuare le transazioni usate come input di ogni transazione.\n",
    "2. Individuare gli indirizzi di tali transazioni.\n",
    "3. Per ogni transazione, collegare il primo indirizzo della prima transazione di input con\n",
    "   tutti gli altri indirizzi delle altre transazioni di input.\n",
    "\n",
    "Utilizziamo il DataFrame `spent_df` creato in precedenza per il calcolo dell'UTXO poiché\n",
    "mette sulla stessa riga due transazioni:\n",
    "\n",
    "- Sulla colonna `tx` troviamo transazioni che riferiscono altre transazioni come loro input.\n",
    "- Sulla colonna `in_tx` troviamo transazioni riferite come input e di cui conosciamo\n",
    "  l'address di destinazione.\n",
    "\n",
    "L'euristica di **clustering** prevede inoltre che si considerino transazioni con più di un\n",
    "input, sarà necessario filtrare il dataset di conseguenza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df = pd.read_csv(\n",
    "    \"mappings.csv\", names=[\"hash\", \"address\"], dtype={\"hash\": str, \"address\": np.uint32}\n",
    ")\n",
    "\n",
    "more_than_1 = spent_df.groupby(\"tx\")[\"in_tx\"].count().reset_index()\n",
    "more_than_1 = more_than_1[more_than_1[\"in_tx\"] > 1].rename(\n",
    "    columns={\"in_tx\": \"in_count\"}\n",
    ")\n",
    "more_than_1 = more_than_1.merge(spent_df, how=\"inner\", on=\"tx\").drop(\n",
    "    columns=[\"in_count\"]\n",
    ")\n",
    "\n",
    "more_than_1 = more_than_1.merge(mapping_df, how=\"inner\", on=\"address\")\n",
    "\n",
    "del spent_df\n",
    "del mapping_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora possiamo popolare il grafo andando a prendere per ogni transazione l'indirizzo del primo\n",
    "input per collegarlo agli indirizzi degli altri input della stessa transazione.\n",
    "\n",
    "Per evitare archi in loop sullo stesso nodo controlliamo che i due nodi siano differenti\n",
    "prima di aggiungere l'arco.\n",
    "\n",
    "Non è necessario controllare che un arco venga aggiunto più volte poiché, se ci provassimo,\n",
    "il risultato sarebbe in quanto il controllo viene già fatto internamente da `NetworkX`.\n",
    "\n",
    "Non ho aggiunto tutti gli address presenti nel dataset di mapping come nodi del grafo:\n",
    "\n",
    "- Per risparmiare memoria.\n",
    "- Molti sarebbero rimasti nodi singoli se non riferiti.\n",
    "\n",
    "Calcoliamo infine le componenti connesse ottenendo i cluster e ordiniamoli per dimensione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr_graph = nx.Graph()\n",
    "addresses = more_than_1.groupby(\"tx\")[\"hash\"].apply(list)\n",
    "for a in addresses:\n",
    "    for i in range(1, len(a)):\n",
    "        if a[0] != a[i]:\n",
    "            addr_graph.add_edge(a[0], a[i])\n",
    "\n",
    "clusters = sorted(nx.connected_components(addr_graph), key=len, reverse=True)\n",
    "\n",
    "del addr_graph\n",
    "del more_than_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi cluster\n",
    "\n",
    "Procediamo ora con qualche analisi sui cluster ottenuti come\n",
    "\n",
    "- Dimensione media, minima e massima dei cluster.\n",
    "- Distribuzione della dimensione dei cluster.\n",
    "\n",
    "Uno degli obbiettivi è quello di mettere in evidenza la distribuzione _power law_ della\n",
    "dimensione dei cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sizes = [len(cluster) for cluster in clusters]\n",
    "\n",
    "print(f\"Numero di cluster trovati: {len(clusters)}\")\n",
    "print(f\"Dimensione media: {np.mean(cluster_sizes):.4f}\")\n",
    "print(f\"Dimensione minima: {cluster_sizes[-1]}\")\n",
    "print(f\"Dimensione massima: {cluster_sizes[0]}\")\n",
    "print(f\"Dimensione dei 10 cluster più grandi: {cluster_sizes[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "\n",
    "Come plot ho optato per\n",
    "\n",
    "- Un istogramma delle frequenze per ciascuna dimensione\n",
    "- Uno scatter plot su scala logaritmica per evidenziare come la dimensione dei cluster\n",
    "  segua una lo stesso pattern di una distribuzione power law, tipica delle reti scale\n",
    "  free.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "ax1.hist(cluster_sizes, ec=\"w\", color=\"darkred\")\n",
    "ax1.set_title(\"Distribuzione della dimensione dei cluster\")\n",
    "ax1.set_xlabel(\"Dimensione cluster\")\n",
    "ax1.set_ylabel(\"Frequenza della dimensione\")\n",
    "ax1.set_yscale(\"log\")\n",
    "\n",
    "cluster_sizes_count = {cs: 0 for cs in cluster_sizes}\n",
    "for cs in cluster_sizes:\n",
    "    cluster_sizes_count[cs] += 1\n",
    "\n",
    "data = list(zip(*[(k, cluster_sizes_count[k]) for k in cluster_sizes_count.keys()]))\n",
    "\n",
    "ax2.scatter(list(data[0]), list(data[1]), marker=\"x\", c=\"chocolate\")\n",
    "ax2.set_xlabel(\"Dimensione cluster\")\n",
    "ax2.set_ylabel(\"Frequenza della dimensione\")\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deanonimizzazione\n",
    "\n",
    "Passiamo alla parte di deanonimizzazione degli indirizzi considerando solo i 10 cluster più\n",
    "grandi.\n",
    "\n",
    "Per entrambi i siti ho utilizzato gli strumenti della libreria `BeautifulSoup` in quanto le\n",
    "pagine fornivano le informazioni rilevanti nel loro codice `html`.\n",
    "\n",
    "### WalletExplorer\n",
    "\n",
    "Ho iniziato dal sito [WalletExplorer](https://www.walletexplorer.com/) che si presenta con\n",
    "una `form` in cui è possibile inserire l'hash dell'indirizzo che cerchiamo.\n",
    "\n",
    "![wallet_explorer1_img](images/wallet_explorer1.png)\n",
    "\n",
    "Il `tag` che cerchiamo è un tag di input con il campo `name` equivalente a `q`. Possiamo\n",
    "dunque effettuare una richiesta `GET` in cui specifichiamo il parametro `params` tramite un\n",
    "dizionario in modo da ottenere una pagina simile a questa\n",
    "\n",
    "![wallet_explorer2_img](images/wallet_explorer2.png)\n",
    "\n",
    "L'informazione che cerchiamo si trova nel primo tag `h2` che troviamo nella pagina.\n",
    "\n",
    "![wallet_explorer4](images/wallet_explorer4.png)\n",
    "\n",
    "Una volta agganciato il tag con `BeautifulSoup` possiamo direttamente ricavare il testo\n",
    "contenuto tramite la funzione `get_text()`.\n",
    "\n",
    "Se la stringa che ricaviamo è un numero racchiuso tramite parentesi quadre significa che non\n",
    "siamo riusciti a deanonimizzare l'indirizzo.\n",
    "\n",
    "Se invece troviamo una parola siamo riusciti a deanonimizzare l'indirizzo e possiamo passare\n",
    "al cluster successivo.\n",
    "\n",
    "#### AntiScraping\n",
    "\n",
    "Nel caso in cui si effetuino troppe richieste in poco tempo alla pagina questa ci blocca\n",
    "ritornando un codice di errore `429`.\n",
    "\n",
    "Per evitare di essere bloccato ho limitato a 10 il numero di richieste per ogni cluster e ho\n",
    "aggiunto un ritardo di 3 secondi tra una richiesta e l'altra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.walletexplorer.com/\"\n",
    "session = requests.Session()\n",
    "session.headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    + \" (KHTML, like Gecko) Chrome/61.0.3163.100Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "def scrape_wallet_explorer(address: str) -> None | str:\n",
    "    r = session.get(base_url, params={\"q\": address})\n",
    "    if r.status_code == 200:\n",
    "        soup = bs4.BeautifulSoup(r.text, \"html.parser\")\n",
    "        wallet_tag = soup.find(\"h2\")\n",
    "        if isinstance(wallet_tag, bs4.Tag):\n",
    "            text = wallet_tag.get_text()\n",
    "            wallet_name = text.split(\" \")[1]\n",
    "            if wallet_name.startswith(\"[\"):\n",
    "                return None\n",
    "            else:\n",
    "                return wallet_name\n",
    "    else:\n",
    "        return f\"Errore {r.status_code}: {r.reason}\"\n",
    "\n",
    "\n",
    "too_many_requests = False\n",
    "for cluster_ID, cluster in enumerate(clusters[:10], start=1):\n",
    "    if too_many_requests == True:\n",
    "        break\n",
    "    for addr_ID, addr in enumerate(cluster):\n",
    "        if addr_ID < 10:\n",
    "            time.sleep(3)\n",
    "            wallet_name = scrape_wallet_explorer(addr)\n",
    "            if wallet_name != None:\n",
    "                if wallet_name.startswith(\"Errore\"):\n",
    "                    too_many_requests = True\n",
    "                print(f\"Cluster {cluster_ID}: {wallet_name}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Cluster {cluster_ID}: Anonimo\")\n",
    "            break\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BitInfoCharts\n",
    "\n",
    "Per la seconda fase di scraping esploriamo il sito [BitInfoCharts]\n",
    "(https://bitinfocharts.com/) la cui URL può essere estesa con\n",
    "\n",
    "`/bitcoin/address/<address hash>`\n",
    "\n",
    "per ottenere una pagina di questo tipo\n",
    "\n",
    "![bit_info_charts1](images/bit_info_charts2.png)\n",
    "\n",
    "Come vediamo all'interno della pagina è presente un tag di tipo `small` in cui troviamo\n",
    "il nome di un _wallet_.\n",
    "\n",
    "Se il nome del wallet non è composto da sole cifre, allora siamo riusciti a deanonimizzare\n",
    "l'indirizzo, in caso contrario dobbiamo compiere altri tentativi.\n",
    "\n",
    "#### Antiscraping\n",
    "\n",
    "Questo sito permette maggiore libertà in fase di scraping, infatti sono riuscito ad inviare\n",
    "anche centinaia di richieste per tentare di deanonimizzare gli indirizzi di uno stesso\n",
    "cluster. Il tutto senza aggiungere ritardo tra l'una e l'altra richiesta.\n",
    "\n",
    "In ogni caso dopo un troppi tentativi si viene bloccati e il blocco può durare anche giorni.\n",
    "Trovo quindi necessario limitare il numero di richieste per poter effettuare più prove.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "base_url = \"https://bitinfocharts.com/bitcoin/address/\"\n",
    "\n",
    "\n",
    "def scrape_bit_info_charts(address: str) -> None | str:\n",
    "    try:\n",
    "        driver.get(base_url + address)\n",
    "        # small_tags = WebDriverWait(driver, 10).until(\n",
    "        #     EC.presence_of_all_elements_located((By.TAG_NAME, \"small\"))\n",
    "        # )\n",
    "        small_tags = driver.find_elements(By.TAG_NAME, \"small\")\n",
    "        for tag in small_tags:\n",
    "            if tag.text.startswith(\"wallet:\"):\n",
    "                return tag.text.split(\" \")[1]\n",
    "    except TimeoutException:\n",
    "        return None\n",
    "\n",
    "\n",
    "http_error = False\n",
    "for cluster_ID, cluster in enumerate(clusters[:10], start=1):\n",
    "    if http_error == True:\n",
    "        break\n",
    "    for addr_ID, addr in enumerate(cluster):\n",
    "        if addr_ID < 10:\n",
    "            # time.sleep(2)\n",
    "            wallet_name = scrape_bit_info_charts(addr)\n",
    "            if wallet_name != None and not wallet_name.isdigit():\n",
    "                if wallet_name.startswith(\"Errore\"):\n",
    "                    http_error = True\n",
    "                    print(wallet_name)\n",
    "                    break\n",
    "                print(f\"Cluster {cluster_ID}: {wallet_name}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Cluster {cluster_ID}: Anonimo\")\n",
    "            break\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://bitinfocharts.com/bitcoin/address/\"\n",
    "session = requests.Session()\n",
    "session.headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    + \" (KHTML, like Gecko) Chrome/61.0.3163.100Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "def scrape_bit_info_charts(address: str) -> None | str:\n",
    "    r = session.get(base_url + address)\n",
    "    if r.status_code == 200:\n",
    "        soup = bs4.BeautifulSoup(r.text, \"html.parser\")\n",
    "        small_tags = soup.find_all(\"small\")\n",
    "        for tag in small_tags:\n",
    "            if isinstance(tag, bs4.Tag):\n",
    "                text = tag.get_text()\n",
    "                if text.startswith(\"wallet:\"):\n",
    "                    return text.split(\" \")[1]\n",
    "\n",
    "        return None\n",
    "    else:\n",
    "        return f\"Errore {r.status_code}: {r.reason}\"\n",
    "\n",
    "\n",
    "http_error = False\n",
    "for cluster_ID, cluster in enumerate(clusters[:10], start=1):\n",
    "    if http_error == True:\n",
    "        break\n",
    "    for addr_ID, addr in enumerate(cluster):\n",
    "        if addr_ID < 10:\n",
    "            # time.sleep(2)\n",
    "            wallet_name = scrape_bit_info_charts(addr)\n",
    "            if wallet_name != None and not wallet_name.isdigit():\n",
    "                if wallet_name.startswith(\"Errore\"):\n",
    "                    http_error = True\n",
    "                    print(wallet_name)\n",
    "                    break\n",
    "                print(f\"Cluster {cluster_ID}: {wallet_name}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Cluster {cluster_ID}: Anonimo\")\n",
    "            break\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per entrambe le pagine ho utilizzato la libreria `requests` unita a `B`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusioni\n",
    "\n",
    "Tramite il sito [WalletExplorer](https://www.walletexplorer.com/) sono riuscito a\n",
    "deanonimizzare 5 indirizzi.\n",
    "\n",
    "- **CoinJoinMess**: servizio per combinare più pagamenti in una singola transazione per\n",
    "  rendere più difficile la deanonimizzazione degli indirizzi.\n",
    "- **SilkRoadMarketplace**: mercato illegale, per lo più di merci di contrabbando\n",
    "- **Instawallet.org**: servizio per effettuare transazioni e garantire un certo grado di\n",
    "  anonimato.\n",
    "- **BTC-e.com-old**: un exchanger.\n",
    "- **BtcDice.com**: servizio di scommesse.\n",
    "\n",
    "Tramite il sito [BitInfoCharts](https://bitinfocharts.com/) sono invece riuscito a\n",
    "deanonimizzare 4 servizi.\n",
    "\n",
    "- **F2Pool**: mining pool.\n",
    "- **SilkRoadMarketplace**: mercato illegale, per lo più di merci di contrabbando.\n",
    "- **Instawallet.org**: servizio per effettuare transazioni e garantire un certo grado di\n",
    "  anonimato.\n",
    "- **Eligius**: mining pool.\n",
    "\n",
    "Il fatto che alcuni cluster vengano deanonimizzati in modo differente dai due siti potrebbe\n",
    "essere imputabile a diversi fattori.\n",
    "\n",
    "I dati potrebbero non essere aggiornati allo stesso modo dai due siti. Potrebbero inoltre\n",
    "essere stati adottati algoritmi ed euristiche differenti i quali potrebbero differire in\n",
    "precisione e dunque portare ad errori di vario genere in alcuni casi.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
